{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z17OmgavQfp4"
   },
   "source": [
    "# Content-Enriched Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quo5K1L5afkO"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- Content-Based Recommendation systems suggest items that have similar features to items that a user has interacted with. \n",
    "\n",
    "- This results in a limited ability to expand on the users' existing interests. To address this shortcoming, content-enriched (hybrid) models have been proposed and leverage both content-features and user-item interactions to make recommendations. \n",
    "\n",
    "- One such approach is the Two-Tower network which uses content-aware features to enhance the collaborative filtering (CF) approaches. \n",
    "\n",
    "- Two-Tower Networks are often leveraged in the retrieval or candidate generation stage of recommender systems in which the initial set of candidates is selected out of all the possible candidates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCeYA79m1DEX"
   },
   "source": [
    "Retrieval models often composed of two sub-models:\n",
    "\n",
    "1. A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
    "2. A candidate model computing the candidate representation (an equally-sized vector) using the candidate features\n",
    "\n",
    "The outputs of the two models are then multiplied together to give us a query-candidate affinity score - the higher the score, the better the match between the candidate and the query. The dot product of a user, item pair that actually interacted is high and the dot product of a non-interacting pair is close to 0.\n",
    "\n",
    "The following notebook implements this two-tower model using [Tensorflow Recommenders](https://www.tensorflow.org/recommenders).\n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*bii7baVk5nF6lulx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7QZ3kkMQo48"
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "The Movielens dataset is a popular dataset from the [GroupLens](https://grouplens.org/datasets/movielens/) research group at the University of Minnesota. It contains a set of ratings given to movies by a set of users, and is often used for recommender system research.\n",
    "\n",
    "In this demo, the data will be treated as an implicit feedback: expressesing which movies the users watched (and rated), and which they did not. Thus, a users' watches gives information about which things they prefer to see and which they'd rather not see. Every movie a user has watched is a positive example and every movie they have not seen is a negative example.\n",
    "\n",
    "This model predicts a set of movies from the catalogue that the user is likely to watch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sawo1x8kQk9b"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SZGYDaF-m5wZ"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PAqjR4a1RR4"
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "We load the MovieLens dataset from [Tensorflow Datasets](https://www.tensorflow.org/datasets). Loading `movielens/100k_ratings` yields a `tf.data.Dataset` object containing the ratings data and loading `movielens/100k_movies` yields a `tf.data.Dataset` object containing only the movies data.\n",
    "\n",
    "<!-- the MovieLens dataset does not have predefined splits, all data are under `train` split. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aaQhqcLGP0jL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 11:03:11.669133: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 32.41 MiB, total: 37.10 MiB) to ../data/movielens/100k-ratings/0.1.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027468204498291016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Dl Completed...",
       "rate": null,
       "total": 0,
       "unit": " url",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9359b5018494dfbb6cc7be86c54b6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029499053955078125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Dl Size...",
       "rate": null,
       "total": 0,
       "unit": " MiB",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be691a31dc7b4e4cbb94626c46d46389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0271909236907959,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extraction completed...",
       "rate": null,
       "total": 0,
       "unit": " file",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13593b0be99436382f9aeb4cddff666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028201580047607422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating splits...",
       "rate": null,
       "total": 1,
       "unit": " splits",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ee822e112744ff9c34fc7fc1634b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020430326461791992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train examples...",
       "rate": null,
       "total": 100000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199ccf9afb7445eab16cbcc697727687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02061605453491211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Shuffling ../data/movielens/100k-ratings/0.1.1.incompleteA6EVJO/movielens-train.tfrecord*...",
       "rate": null,
       "total": 100000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fbbad687f34f8b82f559bb2a340617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ../data/movielens/100k-ratings/0.1.1.incompleteA6EVJO/movielens-train.tfrecord*...:   0%|          |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to ../data/movielens/100k-ratings/0.1.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 11:03:57.679716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-21 11:03:59.045734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13795 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:87:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 150.35 KiB, total: 4.84 MiB) to ../data/movielens/100k-movies/0.1.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029404163360595703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Dl Completed...",
       "rate": null,
       "total": 0,
       "unit": " url",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d9d3c30d544faa6a43813de6b519a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02999114990234375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Dl Size...",
       "rate": null,
       "total": 0,
       "unit": " MiB",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33e5c7e2a7f414a964343d15a9a327c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03075122833251953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extraction completed...",
       "rate": null,
       "total": 0,
       "unit": " file",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6f2171f5894fd583520ad04a06ce82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027445316314697266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating splits...",
       "rate": null,
       "total": 1,
       "unit": " splits",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ad4d0bfa3240719effd172df07732e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02209925651550293,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train examples...",
       "rate": null,
       "total": 1682,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1011ea9d1854b4aa1e3e7e1005d6752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/1682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021764278411865234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Shuffling ../data/movielens/100k-movies/0.1.1.incompleteV0JI3T/movielens-train.tfrecord*...",
       "rate": null,
       "total": 1682,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53c7b912a84414d953259bbcbb88662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ../data/movielens/100k-movies/0.1.1.incompleteV0JI3T/movielens-train.tfrecord*...:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to ../data/movielens/100k-movies/0.1.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Ratings\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\", data_dir=\"../data\")\n",
    "# Features of movies\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\", data_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRHorm8W1yf3"
   },
   "source": [
    "The ratings dataset returns a dictionary of movie id, user id, rating, timestamp, movie information, and user information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_1-KQV2ynMdh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 11:04:01.562475: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bucketized_user_age</th>\n",
       "      <th>movie_genres</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>raw_user_age</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_occupation_label</th>\n",
       "      <th>user_occupation_text</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>user_zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>[7]</td>\n",
       "      <td>b'357'</td>\n",
       "      <td>b\"One Flew Over the Cuckoo's Nest (1975)\"</td>\n",
       "      <td>46.0</td>\n",
       "      <td>879024327</td>\n",
       "      <td>True</td>\n",
       "      <td>b'138'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'doctor'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'53211'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>[4, 14]</td>\n",
       "      <td>b'709'</td>\n",
       "      <td>b'Strictly Ballroom (1992)'</td>\n",
       "      <td>32.0</td>\n",
       "      <td>875654590</td>\n",
       "      <td>True</td>\n",
       "      <td>b'92'</td>\n",
       "      <td>5</td>\n",
       "      <td>b'entertainment'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'80525'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>[4]</td>\n",
       "      <td>b'412'</td>\n",
       "      <td>b'Very Brady Sequel, A (1996)'</td>\n",
       "      <td>24.0</td>\n",
       "      <td>882075110</td>\n",
       "      <td>True</td>\n",
       "      <td>b'301'</td>\n",
       "      <td>17</td>\n",
       "      <td>b'student'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'55439'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[5, 7]</td>\n",
       "      <td>b'56'</td>\n",
       "      <td>b'Pulp Fiction (1994)'</td>\n",
       "      <td>50.0</td>\n",
       "      <td>883326919</td>\n",
       "      <td>True</td>\n",
       "      <td>b'60'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'healthcare'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>b'06472'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>[10, 16]</td>\n",
       "      <td>b'895'</td>\n",
       "      <td>b'Scream 2 (1997)'</td>\n",
       "      <td>55.0</td>\n",
       "      <td>891409199</td>\n",
       "      <td>True</td>\n",
       "      <td>b'197'</td>\n",
       "      <td>18</td>\n",
       "      <td>b'technician'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'75094'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bucketized_user_age movie_genres movie_id  \\\n",
       "0                 45.0          [7]   b'357'   \n",
       "1                 25.0      [4, 14]   b'709'   \n",
       "2                 18.0          [4]   b'412'   \n",
       "3                 50.0       [5, 7]    b'56'   \n",
       "4                 50.0     [10, 16]   b'895'   \n",
       "\n",
       "                                 movie_title  raw_user_age  timestamp  \\\n",
       "0  b\"One Flew Over the Cuckoo's Nest (1975)\"          46.0  879024327   \n",
       "1                b'Strictly Ballroom (1992)'          32.0  875654590   \n",
       "2             b'Very Brady Sequel, A (1996)'          24.0  882075110   \n",
       "3                     b'Pulp Fiction (1994)'          50.0  883326919   \n",
       "4                         b'Scream 2 (1997)'          55.0  891409199   \n",
       "\n",
       "   user_gender user_id  user_occupation_label user_occupation_text  \\\n",
       "0         True  b'138'                      4            b'doctor'   \n",
       "1         True   b'92'                      5     b'entertainment'   \n",
       "2         True  b'301'                     17           b'student'   \n",
       "3         True   b'60'                      4        b'healthcare'   \n",
       "4         True  b'197'                     18        b'technician'   \n",
       "\n",
       "   user_rating user_zip_code  \n",
       "0          4.0      b'53211'  \n",
       "1          2.0      b'80525'  \n",
       "2          4.0      b'55439'  \n",
       "3          4.0      b'06472'  \n",
       "4          3.0      b'75094'  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_data = []\n",
    "for x in ratings.take(5).as_numpy_iterator():\n",
    "  ratings_data.append(x)\n",
    "\n",
    "pd.DataFrame.from_dict(ratings_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGLGCjSt_q96"
   },
   "source": [
    "The movies dataset contains the movie id, movie title, and data on what genres it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kHLsIHhw_x1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 11:04:01.643019: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_genres</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4]</td>\n",
       "      <td>b'1681'</td>\n",
       "      <td>b'You So Crazy (1994)'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>b'1457'</td>\n",
       "      <td>b'Love Is All There Is (1996)'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>b'500'</td>\n",
       "      <td>b'Fly Away Home (1996)'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0]</td>\n",
       "      <td>b'838'</td>\n",
       "      <td>b'In the Line of Duty 2 (1987)'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[7]</td>\n",
       "      <td>b'1648'</td>\n",
       "      <td>b'Niagara, Niagara (1997)'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  movie_genres movie_id                      movie_title\n",
       "0          [4]  b'1681'           b'You So Crazy (1994)'\n",
       "1       [4, 7]  b'1457'   b'Love Is All There Is (1996)'\n",
       "2       [1, 3]   b'500'          b'Fly Away Home (1996)'\n",
       "3          [0]   b'838'  b'In the Line of Duty 2 (1987)'\n",
       "4          [7]  b'1648'       b'Niagara, Niagara (1997)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_data = []\n",
    "for x in movies.take(5).as_numpy_iterator():\n",
    "  movies_data.append(x)\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(movies_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUdT-f4RxMKs"
   },
   "source": [
    "In this example, we will explore the `user_id` and `movie_title` fields in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uhbEvPJqxLec"
   },
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"]\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUDjkzYFaACo"
   },
   "source": [
    "The `user_id` and `movie_title` features are categorical and we need to map them to embedding vectors in order to use them in a deep learning model. Our goal is to predict which user is going to watch which movie. To do that, we represent each user and each movie by an embedding vector. Initially, these embeddings will take on random values - but during training, we will adjust them so that embeddings of users and the movies they watch end up closer together.\n",
    "\n",
    "To turn the raw categorical features into embeddings, we must first translate the values into a range of continuous integers, also known as a mapping or vocabulary. For example, we map the value \"Star Wars\" to 15. Then, we need to convert these integers into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i7qsPEwvZ6aZ"
   },
   "outputs": [],
   "source": [
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCi-seR86qqa"
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z20PyfSXP3Um"
   },
   "source": [
    "### The Query Tower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJYwjpLRaEzj"
   },
   "source": [
    "We start by defining the UserModel which will convert the raw input to feature embeddings using `Embedding` layers.\n",
    "\n",
    "An embedding layer has two dimensions: the first dimension tells us how many distinct categories we can embed; the second tells us how large the vector representing each of them can be.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kHQZJEhXP93N"
   },
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "\n",
    "    return self.user_embedding(inputs[\"user_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG4YFy9SQ08d"
   },
   "source": [
    "### The Candidate Tower\n",
    "\n",
    "We can do the same with the movie tower.\n",
    "\n",
    "In this case, our `movie_title` feature is text, so the first transformations we should apply is tokenization and create a vocabulary of these tokens. The `tf.keras.layers.TextVectorization` layer from Keras performs both these steps. To embed the text, we can average all of the words' embeddings to get a single embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qNUwfIJTQ332"
   },
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "          vocabulary=unique_movie_titles, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
    "    ])\n",
    "\n",
    "    self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens)\n",
    "\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      self.title_vectorizer,\n",
    "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "\n",
    "    self.title_vectorizer.adapt(movies)\n",
    "\n",
    "  def call(self, titles):\n",
    "    return tf.concat([\n",
    "        self.title_embedding(titles),\n",
    "        self.title_text_embedding(titles),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZUFeSlWRHGx"
   },
   "source": [
    "### The Full Model\n",
    "\n",
    "We can now put it all together into a model. TFRS exposes a base model class (`tfrs.models.Model`) which streamlines building models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r10RiPtqVIAl"
   },
   "source": [
    "### Metrics\n",
    "\n",
    "In our training data we have positive (user, movie) pairs. We need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates. If the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "\n",
    "We use the `tfrs.metrics.FactorizedTopK` which has one required argument: the dataset of candidates that are used as implicit negatives for evaluation. In this case, that's the `movies` dataset, converted into embeddings via our movie model. This metric measures how good the model is at picking the true candidate out of all possible candidates in the system.\n",
    "\n",
    "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. These tell us whether the true positive is in the top-k retrieved items from the entire candidate set. For example, a top-5 categorical accuracy metric of 0.2 would tell us that, on average, the true positive is in the top 5 retrieved items 20% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8n7c5CHFp0ow"
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.query_model = tf.keras.Sequential([\n",
    "      UserModel(),\n",
    "      tf.keras.layers.Dense(32)\n",
    "    ])\n",
    "    self.candidate_model = tf.keras.Sequential([\n",
    "      MovieModel(),\n",
    "      tf.keras.layers.Dense(32)\n",
    "    ])\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "        metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(self.candidate_model),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    # We only pass the user id and timestamp features into the query model. This\n",
    "    # is to ensure that the training inputs would have the same keys as the\n",
    "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "    # error when loading the query model after saving it.\n",
    "    query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "    })\n",
    "    movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
    "\n",
    "    return self.task(query_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDN_LJGlnRGo"
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "After defining the model, we use standard Keras fitting and evaluation routines to fit and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Az_GWALtzU01"
   },
   "outputs": [],
   "source": [
    "# Spslit the data into a training and a test set.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wD9nRYrhtwvY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 16s 271ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0092 - factorized_top_k/top_5_categorical_accuracy: 0.0172 - factorized_top_k/top_10_categorical_accuracy: 0.0256 - factorized_top_k/top_50_categorical_accuracy: 0.0824 - factorized_top_k/top_100_categorical_accuracy: 0.1473 - loss: 14579.4626 - regularization_loss: 0.0000e+00 - total_loss: 14579.4626\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 12s 270ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0020 - factorized_top_k/top_5_categorical_accuracy: 0.0126 - factorized_top_k/top_10_categorical_accuracy: 0.0251 - factorized_top_k/top_50_categorical_accuracy: 0.1129 - factorized_top_k/top_100_categorical_accuracy: 0.2133 - loss: 14136.2136 - regularization_loss: 0.0000e+00 - total_loss: 14136.2136\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 13s 275ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0021 - factorized_top_k/top_5_categorical_accuracy: 0.0155 - factorized_top_k/top_10_categorical_accuracy: 0.0307 - factorized_top_k/top_50_categorical_accuracy: 0.1389 - factorized_top_k/top_100_categorical_accuracy: 0.2535 - loss: 13939.9264 - regularization_loss: 0.0000e+00 - total_loss: 13939.9264\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 13s 271ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0039 - factorized_top_k/top_5_categorical_accuracy: 0.0227 - factorized_top_k/top_10_categorical_accuracy: 0.0429 - factorized_top_k/top_50_categorical_accuracy: 0.1733 - factorized_top_k/top_100_categorical_accuracy: 0.2944 - loss: 13711.3803 - regularization_loss: 0.0000e+00 - total_loss: 13711.3803\n",
      "5/5 [==============================] - 4s 413ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0012 - factorized_top_k/top_5_categorical_accuracy: 0.0077 - factorized_top_k/top_10_categorical_accuracy: 0.0185 - factorized_top_k/top_50_categorical_accuracy: 0.1049 - factorized_top_k/top_100_categorical_accuracy: 0.2132 - loss: 30995.8991 - regularization_loss: 0.0000e+00 - total_loss: 30995.8991\n",
      "Top-100 accuracy (train): 0.29.\n",
      "Top-100 accuracy (test): 0.21.\n"
     ]
    }
   ],
   "source": [
    "baseline_model = MovielensModel()\n",
    "baseline_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "baseline_model.fit(cached_train, epochs=3)\n",
    "\n",
    "baseline_train_accuracy = baseline_model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "baseline_test_accuracy = baseline_model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"Top-100 accuracy (train): {baseline_train_accuracy:.2f}.\")\n",
    "print(f\"Top-100 accuracy (test): {baseline_test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results in Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEiV13gg3IFK"
   },
   "source": [
    "Two-Tower Networks use content-aware features to enhance the collaborative filtering (CF) approaches.\n",
    "\n",
    "In a study conducted on e-commerce data from e-bay, this approach was shown to have improved results over an existing Recently-Viewed-Item (RVI) method. The existing method recommends items that a user has recently viewed ranked by the viewed item’s recency; RVI is widely used as a way of generating personalized recommendations in production systems. It is also a difficult baseline to beat in terms of generating user engagement, given that these are items the user has engaged with recently. It has also been shown to outperform CF-based\n",
    "methods.\n",
    "\n",
    "This method outperformed the RVI method in several Recall@k metrics that were\n",
    "measured. This shows that the model is able to generate appropriate personalized recommendations based on the user’s current shopping mission, and potentially inspire new ones given the right user history.\n",
    "\n",
    "In an online evaluation, initial A/B test results showed that compared to the current personalized recommendation module in production, the proposed method increases the conversion rate by ∼6% to generate recommendations for 90% of listing page impressions.\n",
    "\n",
    "Read more about this study [here](https://arxiv.org/pdf/2102.06156.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGZ5Le4Opgd9"
   },
   "source": [
    "For more information about Tensorflow Recommenders, read more [here](https://www.tensorflow.org/recommenders/examples/basic_retrieval) and for more information about Two-Tower Models, read [here](https://research.google/pubs/pub48840/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "two_tower_recsys_demo.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
